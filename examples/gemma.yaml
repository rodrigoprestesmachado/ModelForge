# ModelForge - Fine-tuning Gemma 3 1B IT com CPC 2015 Brasil
# Configuração otimizada para Google Colab com GPU Tesla T4 (14GB)
# IMPORTANTE: Com LoRA habilitado, device_map será desabilitado automaticamente

# ============================================================================
# CONFIGURAÇÃO DO MODELO
# ============================================================================
model:
  name: "google/gemma-3-1b-it"
  version: "latest"
  repository: "huggingface"
  framework: "pytorch"
  task: "text-generation"

# ============================================================================
# CONFIGURAÇÃO DO DATASET
# ============================================================================
dataset:
  name: "0rakul0/cpc_2015_brasil"
  repository: "huggingface"
  
  splits:
    train: "train"
    # validation: "validation"  # Descomente se o dataset tiver split de validação
  
  columns:
    text: "text"  # Ajuste conforme a estrutura real do dataset
  
  preprocessing:
    max_length: 512  # Adequado para Gemma 1B e economiza memória
    truncation: true
    padding: "max_length"
    return_tensors: "pt"
  
  streaming: false

# ============================================================================
# CONFIGURAÇÃO DE TREINAMENTO
# Otimizado para GPU Tesla T4 (14GB) no Colab
# ============================================================================
training:
  batch_size: 1
  learning_rate: 2.0e-5
  epochs: 3
  scheduler: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_accumulation_steps: 16  # Batch efetivo = 16 (reduzido de 32 para evitar problemas)
  fp16: true
  bf16: false
  max_grad_norm: 1.0
  seed: 42
  gradient_checkpointing: true  # Ativado para economizar memória

# ============================================================================
# CONFIGURAÇÃO DE LoRA (Recomendado para economizar memória)
# IMPORTANTE: Com LoRA habilitado, device_map será desabilitado automaticamente
# para evitar problemas com gradientes durante o treinamento
# ============================================================================
lora:
  enabled: true
  r: 8  # Aumentado de 4 para 8 (melhor capacidade de adaptação)
  lora_alpha: 16  # Geralmente 2x o r
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules: null  # Auto-detect para Gemma (q_proj, k_proj, v_proj, o_proj)

# ============================================================================
# CONFIGURAÇÃO DE AVALIAÇÃO
# ============================================================================
evaluation:
  metrics:
    - "perplexity"
  save_strategy: "epoch"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# ============================================================================
# CONFIGURAÇÃO DE CHECKPOINTS
# ============================================================================
checkpoints:
  save_dir: "/content/checkpoints"
  max_to_keep: 2

# ============================================================================
# CONFIGURAÇÃO DE VERSIONAMENTO
# ============================================================================
versioning:
  hub_name: "seu-username/gemma-3-1b-cpc-brasil"  # CORRIGIDO: agora reflete o modelo 1B
  push_to_hub: false  # Mude para true se quiser fazer push automático
  private: true
  hub_strategy: "end"

# ============================================================================
# CONFIGURAÇÃO DE INFRAESTRUTURA
# ============================================================================
infrastructure:
  type: "colab"
  resources:
    gpu: true
    gpu_count: 1

# ============================================================================
# CREDENCIAIS
# Configure a variável de ambiente HF_TOKEN no Colab:
# import os
# os.environ['HF_TOKEN'] = 'seu_token_aqui'
# OU use Secrets: from google.colab import userdata
# os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')
# ============================================================================
credentials:
  huggingface_token: "${HF_TOKEN}"

# ============================================================================
# LOGGING
# ============================================================================
logging:
  level: "INFO"
  format: "text"  # "text" é mais legível que "json" para desenvolvimento