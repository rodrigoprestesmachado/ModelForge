# ModelForge - Fine-tuning Gemma 3 4B IT com CPC 2015 Brasil
# Configuração para treinamento remoto (Google Colab ou Cloud)

# ============================================================================
# CONFIGURAÇÃO DO MODELO
# ============================================================================
model:
  # Gemma 3 4B Instruction-Tuned
  name: "google/gemma-3-4b-it"
  version: "latest"
  repository: "huggingface"
  framework: "pytorch"
  
  # Para modelos de geração de texto
  task: "text-generation"
  
  # Revisão específica (opcional)
  # revision: "main"

# ============================================================================
# CONFIGURAÇÃO DO DATASET
# ============================================================================
dataset:
  # Dataset CPC 2015 Brasil
  name: "0rakul0/cpc_2015_brasil"
  repository: "huggingface"
  
  # Configuração de splits
  splits:
    train: "train"
    # validation: "validation"  # Descomente se houver split de validação
  
  # Mapeamento de colunas - AJUSTE conforme a estrutura do dataset
  # Verifique as colunas do dataset em: https://huggingface.co/datasets/0rakul0/cpc_2015_brasil
  columns:
    text: "text"  # Coluna principal de texto
    # label: "label"  # Descomente se for classificação
  
  # Pré-processamento otimizado para Gemma
  preprocessing:
    max_length: 2048  # Gemma suporta contextos maiores
    truncation: true
    padding: "max_length"
    return_tensors: "pt"
  
  # Streaming para datasets grandes
  streaming: false

# ============================================================================
# CONFIGURAÇÃO DE TREINAMENTO
# Otimizado para GPU com memória limitada (Colab free/pro)
# ============================================================================
training:
  # Batch size pequeno para caber na memória
  batch_size: 1
  
  # Learning rate baixo para fine-tuning
  learning_rate: 2.0e-5
  
  # Número de épocas
  epochs: 3
  
  # Scheduler de learning rate
  scheduler: "cosine"
  
  # Warmup
  warmup_ratio: 0.1
  
  # Regularização
  weight_decay: 0.01
  
  # Acumulação de gradiente para simular batch maior
  # Batch efetivo = batch_size * gradient_accumulation_steps = 1 * 16 = 16
  gradient_accumulation_steps: 16
  
  # Precisão mista para economizar memória e acelerar
  # Use bf16 se disponível (Colab Pro+ com A100)
  fp16: true
  bf16: false
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Seed para reprodutibilidade
  seed: 42

# ============================================================================
# CONFIGURAÇÃO DE AVALIAÇÃO
# ============================================================================
evaluation:
  # Métricas para geração de texto
  metrics:
    - "perplexity"
  
  # Estratégia de salvamento
  save_strategy: "epoch"
  
  # Carregar melhor modelo ao final
  load_best_model_at_end: true
  
  # Métrica para selecionar melhor modelo
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# ============================================================================
# CONFIGURAÇÃO DE CHECKPOINTS
# ============================================================================
checkpoints:
  # Diretório para checkpoints (no Colab, use /content/checkpoints)
  save_dir: "/content/checkpoints"
  
  # Manter apenas os melhores checkpoints
  max_to_keep: 2

# ============================================================================
# CONFIGURAÇÃO DE VERSIONAMENTO
# ============================================================================
versioning:
  # Nome do modelo no Hub (substitua pelo seu username)
  hub_name: "seu-username/gemma-3-4b-cpc-brasil"
  
  # Fazer push automático para o Hub
  push_to_hub: true
  
  # Modelo privado
  private: true
  
  # Estratégia de push
  hub_strategy: "end"  # Push apenas no final

# ============================================================================
# CONFIGURAÇÃO DE INFRAESTRUTURA
# Configurado para Google Colab
# ============================================================================
infrastructure:
  # Tipo: colab para Google Colab
  type: "colab"
  
  # Recursos
  resources:
    gpu: true
    gpu_count: 1
    # memory_gb: 16  # Colab Pro

# ============================================================================
# CREDENCIAIS
# Configure a variável de ambiente HF_TOKEN
# ============================================================================
credentials:
  # Token do Hugging Face (necessário para Gemma)
  huggingface_token: "${HF_TOKEN}"

# ============================================================================
# EXPORTAÇÃO (após treinamento)
# ============================================================================
export:
  format: "docker"
  output_dir: "/content/output"
  
  api:
    framework: "flask"
    endpoints:
      - "chat/completions"
      - "completions"
    port: 8000

# ============================================================================
# LOGGING
# ============================================================================
logging:
  level: "INFO"
  format: "json"